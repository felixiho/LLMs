{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMerpWtAdBr15334QozyhQA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/felixiho/LLMs/blob/main/preprocess_glue_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vU9uYSMTKjWg"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "import inspect\n",
        "\n",
        "def preprocess_glue_dataset(dataset_name, model_checkpoint=\"bert-base-uncased\", max_length=128):\n",
        "    \"\"\"\n",
        "    Preprocess any GLUE dataset for model training.\n",
        "\n",
        "    Args:\n",
        "        dataset_name (str): Name of the GLUE dataset to load\n",
        "        model_checkpoint (str): Checkpoint of the model to use for tokenization\n",
        "        max_length (int): Maximum sequence length for truncation\n",
        "\n",
        "    Returns:\n",
        "        tokenized_dataset: The preprocessed dataset\n",
        "        data_collator: A data collator with padding\n",
        "        num_labels: Number of labels in the dataset\n",
        "        task_info: Information about the task structure\n",
        "    \"\"\"\n",
        "    # Load dataset\n",
        "    try:\n",
        "        ds = load_dataset(dataset_name)\n",
        "    except ValueError as e:\n",
        "        # If loading directly from huggingface hub fails, try with \"glue/\" prefix\n",
        "        if not dataset_name.startswith(\"glue/\"):\n",
        "            try:\n",
        "                ds = load_dataset(\"glue\", dataset_name.split(\"/\")[-1])\n",
        "            except:\n",
        "                raise ValueError(f\"Could not load dataset: {dataset_name}\")\n",
        "        else:\n",
        "            raise e\n",
        "\n",
        "    # Print dataset structure info\n",
        "    print(f\"Dataset loaded: {dataset_name}\")\n",
        "    print(f\"Available splits: {list(ds.keys())}\")\n",
        "\n",
        "    # Determine the task type and input structure\n",
        "    task_info = detect_glue_task_structure(ds)\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "    # Define tokenizer function based on task structure\n",
        "    def tokenize_function(examples):\n",
        "        # Single sentence tasks (SST-2, CoLA)\n",
        "        if task_info[\"type\"] == \"single_sentence\":\n",
        "            sentence_key = task_info[\"sentence_key\"]\n",
        "            return tokenizer(examples[sentence_key], truncation=True, max_length=max_length)\n",
        "\n",
        "        # Sentence pair tasks (MRPC, QQP, MNLI, etc.)\n",
        "        elif task_info[\"type\"] == \"sentence_pair\":\n",
        "            sentence1_key = task_info[\"sentence1_key\"]\n",
        "            sentence2_key = task_info[\"sentence2_key\"]\n",
        "            return tokenizer(\n",
        "                examples[sentence1_key],\n",
        "                examples[sentence2_key],\n",
        "                truncation=True,\n",
        "                max_length=max_length\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown task type: {task_info['type']}\")\n",
        "\n",
        "    # Tokenize the dataset\n",
        "    tokenized_ds = ds.map(tokenize_function, batched=True)\n",
        "\n",
        "    # Create data collator\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    # Prepare the dataset for training\n",
        "    for split in tokenized_ds.keys():\n",
        "        # Remove unnecessary columns (keep only model inputs, labels, and attention masks)\n",
        "        columns_to_remove = []\n",
        "        for col in tokenized_ds[split].column_names:\n",
        "            # Keep tokenizer outputs and label column\n",
        "            if col not in tokenizer.model_input_names and col != task_info[\"label_key\"]:\n",
        "                columns_to_remove.append(col)\n",
        "\n",
        "        tokenized_ds[split] = tokenized_ds[split].remove_columns(columns_to_remove)\n",
        "\n",
        "        # Rename the label column to \"labels\" if needed\n",
        "        if task_info[\"label_key\"] != \"labels\":\n",
        "            tokenized_ds[split] = tokenized_ds[split].rename_column(task_info[\"label_key\"], \"labels\")\n",
        "\n",
        "    # Format the dataset to return only tensor-convertible data\n",
        "    tokenized_ds = tokenized_ds.with_format(\"torch\", columns=tokenizer.model_input_names + [\"labels\"])\n",
        "\n",
        "    return tokenized_ds, data_collator, task_info[\"num_labels\"], task_info\n",
        "\n",
        "def detect_glue_task_structure(dataset):\n",
        "    \"\"\"\n",
        "    Detect the structure of a GLUE task.\n",
        "\n",
        "    Args:\n",
        "        dataset: A Hugging Face dataset\n",
        "\n",
        "    Returns:\n",
        "        info: A dictionary containing information about the task structure\n",
        "    \"\"\"\n",
        "    # Get a sample split (train or validation)\n",
        "    split_name = \"train\" if \"train\" in dataset else \"validation\"\n",
        "    features = dataset[split_name].features\n",
        "\n",
        "    # Initialize task info\n",
        "    task_info = {\n",
        "        \"type\": None,\n",
        "        \"sentence_key\": None,\n",
        "        \"sentence1_key\": None,\n",
        "        \"sentence2_key\": None,\n",
        "        \"label_key\": None,\n",
        "        \"num_labels\": None\n",
        "    }\n",
        "\n",
        "    # Detect label column and number of labels\n",
        "    for key, feature in features.items():\n",
        "        if hasattr(feature, \"num_classes\"):\n",
        "            task_info[\"label_key\"] = key\n",
        "            task_info[\"num_labels\"] = feature.num_classes\n",
        "            break\n",
        "\n",
        "    # If no ClassLabel feature found, look for a label column\n",
        "    if task_info[\"label_key\"] is None:\n",
        "        label_candidates = [\"label\", \"labels\", \"Label\", \"class\"]\n",
        "        for candidate in label_candidates:\n",
        "            if candidate in features:\n",
        "                task_info[\"label_key\"] = candidate\n",
        "                # Try to infer number of labels\n",
        "                if hasattr(dataset[split_name], \"info\") and hasattr(dataset[split_name].info, \"features\"):\n",
        "                    if hasattr(dataset[split_name].info.features[candidate], \"num_classes\"):\n",
        "                        task_info[\"num_labels\"] = dataset[split_name].info.features[candidate].num_classes\n",
        "\n",
        "                # Default to binary classification if we can't determine\n",
        "                if task_info[\"num_labels\"] is None:\n",
        "                    unique_labels = set(dataset[split_name][candidate])\n",
        "                    task_info[\"num_labels\"] = len(unique_labels)\n",
        "\n",
        "                break\n",
        "\n",
        "    # Default to binary classification if we still can't determine\n",
        "    if task_info[\"num_labels\"] is None:\n",
        "        task_info[\"num_labels\"] = 2\n",
        "\n",
        "    # Detect task type based on column names\n",
        "    text_columns = []\n",
        "    for key in features.keys():\n",
        "        if key != task_info[\"label_key\"] and features[key].dtype == \"string\":\n",
        "            text_columns.append(key)\n",
        "\n",
        "    # Check for common GLUE text column patterns\n",
        "    sentence_candidates = [\"sentence\", \"text\", \"premise\", \"question\"]\n",
        "    sentence1_candidates = [\"sentence1\", \"premise\", \"question1\", \"question\"]\n",
        "    sentence2_candidates = [\"sentence2\", \"hypothesis\", \"question2\", \"answer\"]\n",
        "\n",
        "    # Detect single sentence tasks\n",
        "    if len(text_columns) == 1:\n",
        "        task_info[\"type\"] = \"single_sentence\"\n",
        "        task_info[\"sentence_key\"] = text_columns[0]\n",
        "\n",
        "    # Detect sentence pair tasks\n",
        "    elif len(text_columns) == 2:\n",
        "        task_info[\"type\"] = \"sentence_pair\"\n",
        "\n",
        "        # Try to identify which column is sentence1 and which is sentence2\n",
        "        s1_found = False\n",
        "        for s1 in sentence1_candidates:\n",
        "            if s1 in text_columns:\n",
        "                task_info[\"sentence1_key\"] = s1\n",
        "                s1_found = True\n",
        "                break\n",
        "\n",
        "        s2_found = False\n",
        "        for s2 in sentence2_candidates:\n",
        "            if s2 in text_columns:\n",
        "                task_info[\"sentence2_key\"] = s2\n",
        "                s2_found = True\n",
        "                break\n",
        "\n",
        "        # If we couldn't identify by name, use the first and second text columns\n",
        "        if not s1_found:\n",
        "            task_info[\"sentence1_key\"] = text_columns[0]\n",
        "\n",
        "        if not s2_found:\n",
        "            task_info[\"sentence2_key\"] = text_columns[1]\n",
        "\n",
        "    # Handle unknown cases\n",
        "    else:\n",
        "        # Default to the first text-like column if available\n",
        "        for key in features.keys():\n",
        "            if key != task_info[\"label_key\"] and \"sentence\" in key.lower():\n",
        "                task_info[\"type\"] = \"single_sentence\"\n",
        "                task_info[\"sentence_key\"] = key\n",
        "                break\n",
        "            elif key != task_info[\"label_key\"] and features[key].dtype == \"string\":\n",
        "                task_info[\"type\"] = \"single_sentence\"\n",
        "                task_info[\"sentence_key\"] = key\n",
        "                break\n",
        "\n",
        "        # If still no task type, raise error\n",
        "        if task_info[\"type\"] is None:\n",
        "            raise ValueError(f\"Could not determine task type from columns: {list(features.keys())}\")\n",
        "\n",
        "    return task_info\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Example with SST-2 (sentiment analysis - single sentence)\n",
        "    dataset_name = \"gimmaru/glue-sst2\"\n",
        "    tokenized_ds, data_collator, num_labels, task_info = preprocess_glue_dataset(dataset_name)\n",
        "\n",
        "    print(f\"\\nPreprocessed {dataset_name}\")\n",
        "    print(f\"Number of labels: {num_labels}\")\n",
        "    print(f\"Task type: {task_info['type']}\")\n",
        "    print(f\"Task structure: {task_info}\")\n",
        "    for split in tokenized_ds:\n",
        "        print(f\"Split {split}: {len(tokenized_ds[split])} examples\")\n",
        "\n",
        "    # Show sample data\n",
        "    samples = tokenized_ds[\"validation\"][:2]\n",
        "    samples_dict = {k: v for k, v in samples.items()}\n",
        "    print(\"\\nSample data structure:\")\n",
        "    for k, v in samples_dict.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "\n",
        "    # Show batch shape\n",
        "    # Filter out non-tensor items and prepare batch items properly\n",
        "    batch_items = []\n",
        "    for i in range(2):\n",
        "        item = {k: v[i] for k, v in samples_dict.items() if k in ['input_ids', 'attention_mask', 'token_type_ids', 'labels']}\n",
        "        batch_items.append(item)\n",
        "\n",
        "    batch = data_collator(batch_items)\n",
        "    print(\"\\nBatch shapes:\")\n",
        "    for k, v in batch.items():\n",
        "        print(f\"{k}: {v.shape}\")\n",
        "\n",
        "    # Example with MRPC (paraphrase detection - sentence pair)\n",
        "    dataset_name = \"SetFit/mrpc\"\n",
        "    tokenized_ds, data_collator, num_labels, task_info = preprocess_glue_dataset(dataset_name)\n",
        "\n",
        "    print(f\"\\nPreprocessed {dataset_name}\")\n",
        "    print(f\"Number of labels: {num_labels}\")\n",
        "    for split in tokenized_ds:\n",
        "        print(f\"Split {split}: {len(tokenized_ds[split])} examples\")"
      ],
      "metadata": {
        "id": "ag8XtFKTL99m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}