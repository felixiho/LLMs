{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPkTuM+tNBkWvBwJv6Cra79",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/felixiho/LLMs/blob/main/Transaction_Compliance_Monitoring_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transactions Compliance Monitoring With Document Injestion"
      ],
      "metadata": {
        "id": "EIgyBp6-_9Kq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --quiet datasets pymongo langchain-mongodb langgraph-checkpoint-mongodb langchain-core langchain-huggingface langgraph pypdf python-docx unstructured pydantic voyageai transformers torch accelerator"
      ],
      "metadata": {
        "collapsed": true,
        "id": "uF728juUAMs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "def set_env_variables(variable):\n",
        "  value = getpass.getpass(f\"Enter the value for {variable}:\")\n",
        "  if len(value):\n",
        "    os.environ[variable] = value\n",
        "\n",
        "# os.environ[\"MONGO_DB_URI\"]\n",
        "# set_env_variables(\"TEST_VARIABLE\")\n",
        "\n",
        "\n",
        "set_env_variables(\"MONGO_DB_URI\")"
      ],
      "metadata": {
        "id": "GP_06xfOAQtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mongo DB Setup"
      ],
      "metadata": {
        "id": "tbm2REQSDIzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pymongo.mongo_client import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "\n",
        "db_uri = os.environ.get(\"MONGO_DB_URI\")\n",
        "if not db_uri:\n",
        "  raise ValueError(\"MONGO_DB_URI environment variable is not set\")\n",
        "\n",
        "client = MongoClient(\n",
        "    db_uri,\n",
        "    server_api=ServerApi('1'),\n",
        "    appname=\"transaction_monitoring_with_document_injestion\"\n",
        ")\n",
        "\n",
        "DB_NAME = \"transaction_compliance\"\n",
        "TRANSACTIONS = \"transactions\"\n",
        "REGULATIONS = \"regulations\"\n",
        "CHECKPOINTS = \"checkpoints\"\n",
        "CHECKPOINTS_WRITES = \"checkpoints_writes\"\n",
        "\n",
        "try:\n",
        "    client.admin.command('ping')\n",
        "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
        "except Exception as e:\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "lAhxtFTkDLFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create MongoDB Collections for:\n",
        "\n",
        "\n",
        "1.   Transactions\n",
        "2.   Regulations\n",
        "3.   Checkpoints\n",
        "4.   Checkpoints Writes\n",
        "\n"
      ],
      "metadata": {
        "id": "v2HG0tWaIvRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db = client[DB_NAME]\n",
        "transaction_collection = db[TRANSACTIONS]\n",
        "regulations_collection = db[REGULATIONS]\n",
        "checkpoints_collection = db[CHECKPOINTS]\n",
        "checkpoints_writes_collection = db[CHECKPOINTS_WRITES]\n",
        "\n",
        "def create_mongodb_collections():\n",
        "  existing_collections = db.list_collection_names()\n",
        "\n",
        "  # Transactions collections\n",
        "  if TRANSACTIONS not in existing_collections:\n",
        "    db.create_collection(\n",
        "        TRANSACTIONS,\n",
        "        validator={\n",
        "            \"$jsonSchema\": {\n",
        "                \"bsonType\": \"object\",\n",
        "                \"required\": [\n",
        "                    'transaction_id',\n",
        "                    'amount',\n",
        "                    'currency',\n",
        "                    'sender',\n",
        "                    'receiver',\n",
        "                    'transaction_date'\n",
        "                ],\n",
        "                \"properties\": {\n",
        "                    \"transaction_id\": {\"bsonType\": \"string\"},\n",
        "                    \"amount\": {\"bsonType\": \"double\", \"minimum\": 0},\n",
        "                    \"currency\": {\"bsonType\": \"string\"},\n",
        "                    \"sender\": {\"bsonType\": \"string\"},\n",
        "                    \"receiver\": {\"bsonType\": \"string\"},\n",
        "                    \"compliance_status\": {\"bsonType\": \"string\"}\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        validationLevel=\"moderate\",\n",
        "    )\n",
        "    print(f\"Collection {TRANSACTIONS} created successfully\")\n",
        "  else:\n",
        "    print(f\"Collection {TRANSACTIONS} already exists\")\n",
        "\n",
        "\n",
        "  # Regulations collections\n",
        "  if REGULATIONS not in existing_collections:\n",
        "    db.create_collection(REGULATIONS)\n",
        "    print(f\"Collection {REGULATIONS} created successfully\")\n",
        "  else:\n",
        "    print(f\"Collection {REGULATIONS} already exists\")\n",
        "\n",
        "\n",
        "  # Checkpoints collections\n",
        "  if CHECKPOINTS not in existing_collections:\n",
        "    db.create_collection(CHECKPOINTS)\n",
        "    print(f\"Collection {CHECKPOINTS} created successfully\")\n",
        "  else:\n",
        "    print(f\"Collection {CHECKPOINTS} already exists\")\n",
        "\n",
        "  # Checkpoint Writes collection\n",
        "  if CHECKPOINTS_WRITES not in existing_collections:\n",
        "    db.create_collection(CHECKPOINTS_WRITES)\n",
        "    print(f\"Collection {CHECKPOINTS_WRITES} created successfully\")\n",
        "\n",
        "\n",
        "\n",
        "create_mongodb_collections()"
      ],
      "metadata": {
        "id": "IV0H80Q2I4AE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector Search Index Creation\n",
        "\n",
        "In other to enable semantic search with embedded documents vectors, we need to\n",
        "enable a vector search index on the regulations collection.\n",
        "\n",
        "We're using cosine similarity because we're doing semantic search and document classification problems.\n",
        "\n",
        "see this for https://www.pinecone.io/learn/vector-similarity/\n",
        "\n",
        "\n",
        "We also poll for readyness after creating the search index as attempting search on unready index causes error"
      ],
      "metadata": {
        "id": "51Cg27WTLkdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from pymongo.operations import SearchIndexModel\n",
        "\n",
        "VECTOR_INDEX_NAME = \"regulation_vector_index\"\n",
        "\n",
        "def create_vector_search_index():\n",
        "  try:\n",
        "    existing_regulation_indexes = regulations_collection.list_search_indexes()\n",
        "    for index in existing_regulation_indexes:\n",
        "      if index[\"name\"] == VECTOR_INDEX_NAME:\n",
        "        print(f\"Vector search index {VECTOR_INDEX_NAME} already exists\")\n",
        "        return\n",
        "  except Exception as e:\n",
        "    print(f\"Error getting indexes: {e}\")\n",
        "    return\n",
        "\n",
        "  #create new vector search index\n",
        "  search_index_model = SearchIndexModel(\n",
        "      definition={\n",
        "          \"fields\": [\n",
        "              {\n",
        "                  \"type\": \"vector\",\n",
        "                  \"path\": \"embedding\",\n",
        "                  \"similarity\": \"cosine\",\n",
        "                  \"numDimensions\": 1024\n",
        "              }\n",
        "          ]\n",
        "      },\n",
        "      name=VECTOR_INDEX_NAME,\n",
        "      type=\"vectorSearch\"\n",
        "  )\n",
        "\n",
        "  try:\n",
        "    new_search_index = regulations_collection.create_search_index(search_index_model)\n",
        "    print(f\"Vector search index {VECTOR_INDEX_NAME} created successfully and is building\")\n",
        "  except Exception as e:\n",
        "    print(f\"Error creating vector search index: {e}\")\n",
        "\n",
        "  #wait for sync\n",
        "  print(\"Polling to check if vector search index is ready\")\n",
        "  predicate = lambda i: i.get(\"queryable\") is True\n",
        "\n",
        "  while True:\n",
        "    try:\n",
        "      indexes = list(regulations_collection.list_search_indexes(new_search_index))\n",
        "      if indexes:\n",
        "        if predicate(indexes[0]):\n",
        "          break\n",
        "      time.sleep(5)\n",
        "    except Exception as e:\n",
        "      print(f\"Error polling for vector search index: {e}\")\n",
        "      break\n",
        "\n",
        "  print(f\"{new_search_index} is ready for querying.\")\n",
        "\n",
        "create_vector_search_index()"
      ],
      "metadata": {
        "id": "aCxGV_uJLa9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Document Processing and Schema Definition\n",
        "\n"
      ],
      "metadata": {
        "id": "-hT8iBRYWVLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "from docx import Document\n",
        "from pydantic import BaseModel, Field\n",
        "from pypdf import PdfReader\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "\n",
        "class RegulationDocument(BaseModel):\n",
        "  # Schema for regulatory Documents\n",
        "\n",
        "  id: Optional[str] = None\n",
        "  title: str\n",
        "  content: str\n",
        "  source: str\n",
        "  document_type: str\n",
        "  jurisdiction: str\n",
        "  publication_date: str\n",
        "  tags: List[str] = Field(default_factory=list)\n",
        "  embedding: Optional[List[float]] = None\n",
        "  chunks: Optional[List[Dict[str, Any]]] = None\n",
        "\n",
        "  def to_dict(self):\n",
        "    return self.model_dump(exclude_none=True)\n",
        "\n",
        "\n",
        "\n",
        "class DocumentProcessor:\n",
        "  \"\"\"Processes different document formats and extracts text\"\"\"\n",
        "\n",
        "  @staticmethod\n",
        "  def extract_text_from_pdf(file_path_or_bytes):\n",
        "    if isinstance(file_path_or_bytes, str):\n",
        "      reader = PdfReader(file_path_or_bytes)\n",
        "    else:\n",
        "      reader = PdfReader(io.BytesIO(file_path_or_bytes))\n",
        "\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "      text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "  @staticmethod\n",
        "  def extract_text_from_docx(file_path_or_bytes):\n",
        "    if isinstance(file_path_or_bytes, str):\n",
        "      document = Document(file_path_or_bytes)\n",
        "    else:\n",
        "      document = Document(io.BytesIO(file_path_or_bytes))\n",
        "\n",
        "    text = \"\"\n",
        "    for paragraph in document.paragraphs:\n",
        "      text += paragraph + \"\\n\"\n",
        "    return text\n",
        "\n",
        "  @staticmethod\n",
        "  def extract_text_from_txt(file_path_or_bytes):\n",
        "    if isinstance(file_path_or_bytes, str):\n",
        "      with open(file_path_or_bytes, encoding=\"utf-8\") as f:\n",
        "        return f.read()\n",
        "    else:\n",
        "      return file_path_or_bytes.decode(\"utf-8\")\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def process_document(file_path, metadata=None):\n",
        "    if metadata is None:\n",
        "      metadata = {}\n",
        "\n",
        "    file_extension = file_path.split('.')[-1].lower()\n",
        "\n",
        "    if file_extension == \"pdf\":\n",
        "      text = DocumentProcessor.extract_text_from_pdf(file_path)\n",
        "      doc_type = \"pdf\"\n",
        "    elif file_extension == \"docx\":\n",
        "      text = DocumentProcessor.extract_text_from_docx(file_path)\n",
        "      doc_type  = \"docx\"\n",
        "    elif file_extension == \"txt\":\n",
        "      text = DocumentProcessor.extract_text_from_txt(file_path)\n",
        "      doc_type = \"txt\"\n",
        "    else:\n",
        "      raise ValueError(f\"Unsupported file format: {file_extension}\")\n",
        "\n",
        "    if \"title\" not in metadata:\n",
        "      title = os.path.basename(file_path).rsplit('.', 1)[0]\n",
        "      metadata[\"title\"] = title\n",
        "\n",
        "    if \"document_type\" not in metadata:\n",
        "      metadata[\"document_type\"] = doc_type\n",
        "\n",
        "    regulation = RegulationDocument(\n",
        "        title=metadata.get(\"title\", \"\"),\n",
        "        content=text,\n",
        "        source=metadata.get(\"source\", file_path),\n",
        "        document_type=metadata.get(\"document_type\", doc_type),\n",
        "        jurisdiction=metadata.get(\"jurisdiction\", \"Unknown\"),\n",
        "        publication_date=metadata.get(\n",
        "            \"publication_date\", datetime.now().strftime(\"%Y-%m-%d\")\n",
        "        ),\n",
        "        tags=metadata.get(\"tags\", []),\n",
        "    )\n",
        "\n",
        "    return regulation\n",
        "\n",
        "  @staticmethod\n",
        "  def extract_metadata_from_content(content):\n",
        "    metadata = {}\n",
        "\n",
        "    # Extract jurisdiction\n",
        "    jurisdiction_pattern = r\"(?i)jurisdiction[:\\s]+(\\w+(?:\\s+\\w+)*)\"\n",
        "    jurisdiction_match = re.search(jurisdiction_pattern, content)\n",
        "    if jurisdiction_match:\n",
        "        metadata[\"jurisdiction\"] = jurisdiction_match.group(1).strip()\n",
        "\n",
        "    # Extract date\n",
        "    date_pattern = r\"(?i)(?:date|published)[:\\s]+(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}|\\d{4}[/-]\\d{1,2}[/-]\\d{1,2})\"\n",
        "    date_match = re.search(date_pattern, content)\n",
        "    if date_match:\n",
        "        metadata[\"publication_date\"] = date_match.group(1).strip()\n",
        "\n",
        "    # Extract tags\n",
        "    tags_pattern = r\"(?i)(?:keywords|tags)[:\\s]+([\\w\\s,]+)\"\n",
        "    tags_match = re.search(tags_pattern, content)\n",
        "    if tags_match:\n",
        "        tags = [tag.strip() for tag in tags_match.group(1).split(\",\")]\n",
        "        metadata[\"tags\"] = tags\n",
        "\n",
        "    return metadata\n"
      ],
      "metadata": {
        "id": "fIY7SZn_TJg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Processing, Embedding Generation and Storage\n",
        "\n",
        "A few notes:\n",
        "\n",
        "1. We implement both chunk level and document level embeddings.\n",
        "This is because it helps with hierarchical retreivel. This means we have a 2 staged retrivel process where we first find the relevant document before finding the relevant chunks within those documents\n",
        "\n",
        "2. We use langchains RecursiveCharacterTextSplitter to preserve semantics\n",
        "\n",
        "3. Proper rate limiting by tracking time between subsequent calls"
      ],
      "metadata": {
        "id": "Izg3NAA4dgkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_env_variables(\"VOYAGE_API_KEY\")"
      ],
      "metadata": {
        "id": "DQhISEdEfSDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import voyageai\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "class TextProcessor:\n",
        "\n",
        "  last_voyage_call = 0\n",
        "  _instance = None\n",
        "\n",
        "  # use singleton pattern to only have one instance throughout the app's lifetime\n",
        "  def __new__(cls, *args, **kwargs):\n",
        "    if cls._instance is None:\n",
        "      cls._instance = super().__new__(cls)\n",
        "      cls._instance._initialized = False\n",
        "    return cls._instance\n",
        "\n",
        "  def __init__(self, chunk_size=1000, chunk_overlap=200):\n",
        "    if not hasattr(self, '_initialized') or not self._initialized:\n",
        "      self.chunk_size = chunk_size\n",
        "      self.chunk_overlap = chunk_overlap\n",
        "      self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=chunk_size,\n",
        "                chunk_overlap=chunk_overlap,\n",
        "                length_function=len,\n",
        "                separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
        "            )\n",
        "      self.voyage_client = voyageai.Client(api_key=os.environ[\"VOYAGE_API_KEY\"])\n",
        "      self.model_name = \"voyage-3\"\n",
        "      self._initialized = True\n",
        "\n",
        "  def chunk_text(self, text):\n",
        "    print(text)\n",
        "    return self.text_splitter.split_text(text)\n",
        "\n",
        "  def generate_embeddings(self, texts: List[str]):\n",
        "    if not texts:\n",
        "      return []\n",
        "\n",
        "    current_time = time.time()\n",
        "    time_since_last_call = current_time - self.last_voyage_call\n",
        "\n",
        "    if time_since_last_call < 20:\n",
        "      wait_time = 20 - time_since_last_call\n",
        "      print(\n",
        "          f\"Due to rate limiting, we're waiting {wait_time} seconds\"\n",
        "      )\n",
        "      time.sleep(wait_time)\n",
        "    embeddings = self.voyage_client.embed(texts, model=self.model_name).embeddings\n",
        "\n",
        "    self.last_voyage_call = time.time()\n",
        "    return embeddings\n",
        "\n",
        "  def process_document(self, regulation_doc):\n",
        "    chunks = self.chunk_text(regulation_doc.content)\n",
        "    chunk_embeddings = self.generate_embeddings(chunks)\n",
        "    processed_chunks = []\n",
        "\n",
        "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
        "      processed_chunks.append({\n",
        "          \"chunk_id\": i,\n",
        "          \"content\": chunk,\n",
        "          \"embedding\": embedding\n",
        "      })\n",
        "\n",
        "    doc_text = f\"{regulation_doc.title}\\n{chunks[0] if chunks else ''}\"\n",
        "    doc_embedding = self.generate_embeddings(doc_text)[0]\n",
        "\n",
        "    regulation_doc.embedding = doc_embedding\n",
        "    regulation_doc.chunks = processed_chunks\n",
        "\n",
        "    return regulation_doc\n",
        "\n",
        "  def store_regulation(self, regulation_doc):\n",
        "    regulation_dict = regulation_doc.to_dict()\n",
        "\n",
        "    result = regulations_collection.insert_one(regulation_dict)\n",
        "    print(f\"Stored regulation document with ID: {result.inserted_id}\")\n",
        "\n",
        "    return result.inserted_id\n"
      ],
      "metadata": {
        "id": "isuBh4VzdgQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample regulatory texts\n",
        "sample_regulations = [\n",
        "    {\n",
        "        \"title\": \"Anti-Money Laundering Directive\",\n",
        "        \"content\": \"\"\"ANTI-MONEY LAUNDERING DIRECTIVE\n",
        "Jurisdiction: European Union\n",
        "Date: 2021-06-15\n",
        "Keywords: AML, KYC, financial crime, cross-border\n",
        "\n",
        "Section 1: Scope and Definitions\n",
        "1.1 This directive applies to all financial institutions operating within the European Union that process cross-border transactions.\n",
        "1.2 'Cross-border transaction' refers to any financial transfer that originates in one country and terminates in another.\n",
        "1.3 'High-risk jurisdiction' refers to countries identified by the Financial Action Task Force (FATF) as having strategic deficiencies in their AML/CFT regimes.\n",
        "\n",
        "Section 2: Due Diligence Requirements\n",
        "2.1 Enhanced due diligence must be performed for all transactions exceeding €10,000 that involve high-risk jurisdictions.\n",
        "2.2 Financial institutions must verify the identity of both the sender and recipient for all cross-border transactions exceeding €3,000.\n",
        "2.3 For transactions with sanctioned countries, prior approval must be obtained from the compliance department.\n",
        "\n",
        "Section 3: Reporting Requirements\n",
        "3.1 All suspicious transactions must be reported to the national Financial Intelligence Unit within 24 hours of detection.\n",
        "3.2 Monthly reports must be submitted detailing all cross-border transactions exceeding €50,000.\n",
        "3.3 Failure to report suspicious activities may result in fines of up to €5 million or 10% of annual turnover.\n",
        "\"\"\",\n",
        "        \"source\": \"EU Financial Regulatory Authority\",\n",
        "        \"document_type\": \"directive\",\n",
        "        \"jurisdiction\": \"European Union\",\n",
        "        \"publication_date\": \"2021-06-15\",\n",
        "        \"tags\": [\"AML\", \"KYC\", \"financial crime\", \"cross-border\"],\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"Sanctions Compliance Framework\",\n",
        "        \"content\": \"\"\"SANCTIONS COMPLIANCE FRAMEWORK\n",
        "Jurisdiction: United States\n",
        "Date: 2022-03-10\n",
        "Keywords: sanctions, OFAC, restricted parties, compliance\n",
        "\n",
        "Section 1: Overview\n",
        "1.1 This framework outlines compliance requirements for financial institutions regarding transactions subject to sanctions administered by the Office of Foreign Assets Control (OFAC).\n",
        "1.2 All US financial institutions and their foreign branches must comply with these requirements.\n",
        "\n",
        "Section 2: Prohibited Transactions\n",
        "2.1 No financial institution shall process transactions involving entities listed on the Specially Designated Nationals (SDN) list.\n",
        "2.2 Transactions with entities in comprehensively sanctioned countries including Iran, North Korea, Syria, Cuba, and the Crimea region are prohibited without specific OFAC authorization.\n",
        "2.3 Transactions that attempt to circumvent sanctions through third-party intermediaries are strictly prohibited and subject to severe penalties.\n",
        "\n",
        "Section 3: Screening Requirements\n",
        "3.1 All parties to a transaction must be screened against the most current OFAC sanctions lists prior to processing.\n",
        "3.2 Screening must include beneficial owners with 25% or greater ownership interest.\n",
        "3.3 Institutions must implement real-time screening for all international wire transfers regardless of amount.\n",
        "\n",
        "Section 4: Penalties for Non-Compliance\n",
        "4.1 Civil penalties may reach the greater of $1,000,000 per violation or twice the value of the transaction.\n",
        "4.2 Criminal penalties for willful violations may include fines up to $20 million and imprisonment up to 30 years.\n",
        "4.3 Financial institutions may be subject to regulatory actions including restrictions on activities or loss of licenses.\n",
        "\"\"\",\n",
        "        \"source\": \"US Department of Treasury\",\n",
        "        \"document_type\": \"framework\",\n",
        "        \"jurisdiction\": \"United States\",\n",
        "        \"publication_date\": \"2022-03-10\",\n",
        "        \"tags\": [\"sanctions\", \"OFAC\", \"restricted parties\", \"compliance\"],\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "wquJXgfcm355"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_processor = TextProcessor()\n",
        "\n",
        "for reg_data in sample_regulations:\n",
        "  regulation = RegulationDocument(**reg_data)\n",
        "\n",
        "  processed_regulation = text_processor.process_document(regulation)\n",
        "  regulation_id = text_processor.store_regulation(processed_regulation)\n",
        "\n",
        "  print(f\"Processed and stored regulation: {regulation.title}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eduMThZplBod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transaction Data Models and Compliance Status\n",
        "\n"
      ],
      "metadata": {
        "id": "BbbvQaTHppxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "from typing import Any, Dict, Optional\n",
        "\n",
        "from pydantic import BaseModel,  field_validator\n",
        "\n",
        "class ComplianceStaus(str, Enum):\n",
        "  COMPLIANT = \"Compliant\"\n",
        "  REPORTING_REQUIRED = \"Reporting Required\"\n",
        "  VIOLATION = \"Violation\"\n",
        "  PENDING = \"Pending Assessment\"\n",
        "\n",
        "\n",
        "class TransactionParty(BaseModel):\n",
        "  name: str\n",
        "  country: str\n",
        "  account_number: str\n",
        "  institution: str\n",
        "  is_sanctioned: bool = False\n",
        "  risk_score: Optional[float] = None\n",
        "\n",
        "class Transaction(BaseModel):\n",
        "  id: Optional[str] = None\n",
        "  transaction_id: str\n",
        "  amount: float\n",
        "  currency: str\n",
        "  sender: TransactionParty\n",
        "  receiver: TransactionParty\n",
        "  transaction_date: str\n",
        "  transaction_type: str\n",
        "  description: str\n",
        "  compliance_status: ComplianceStaus = ComplianceStaus.PENDING\n",
        "  compliance_details: Optional[Dict[str, Any]] = None\n",
        "\n",
        "  @field_validator(\"amount\")\n",
        "  def amount_must_be_positive(cls, v):\n",
        "    if v <= 0:\n",
        "      raise ValueError(\"Amount must be positive\")\n",
        "    return v\n",
        "\n",
        "  def to_dict(self):\n",
        "    return self.model_dump(exclude_none=True)\n",
        "\n",
        "  def to_prompt(self):\n",
        "    return f\"\"\"Transaction Details:\n",
        "      - Transaction ID: {self.transaction_id}\n",
        "      - Amount: {self.amount} {self.currency}\n",
        "      - Date: {self.transaction_date}\n",
        "      - Type: {self.transaction_type}\n",
        "      - Description: {self.description}\n",
        "\n",
        "      Sender Information:\n",
        "      - Name: {self.sender.name}\n",
        "      - Country: {self.sender.country}\n",
        "      - Institution: {self.sender.institution}\n",
        "      - Sanctioned: {self.sender.is_sanctioned}\n",
        "\n",
        "      Receiver Information:\n",
        "      - Name: {self.receiver.name}\n",
        "      - Country: {self.receiver.country}\n",
        "      - Institution: {self.receiver.institution}\n",
        "      - Sanctioned: {self.receiver.is_sanctioned}\n",
        "    \"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "wofSSfJHpwYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compliance monitoring engine and workflow orchestrator\n",
        "\n",
        "We use the following:\n",
        "\n",
        "1. HF ShiledGemma: For classification\n",
        "2. MongoDb Atlas for vector search, checkpoints and transaction storage\n",
        "3. Voyage AI for generating text embeddings\n",
        "4. LangGraph for stateful  workflow orchestration\n",
        "\n",
        "\n",
        "Compliance Engine:\n",
        "1. Retrieves relevant regulation associated with transaction via vector ssearch\n",
        "2. checks compliance assesment using LLM and returns JSON\n",
        "3. Normalize confidence score using an activation fn -> softmax\n",
        "4. updates transaction records with compliance status and details\n",
        "\n",
        "\n",
        "Compliance Workflow:\n",
        "1. Defines a LangGraph based workflow for processing transactions\n",
        "2. Includes checkpointing, error handling and conditional retries\n"
      ],
      "metadata": {
        "id": "dsMZiH9qrdhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_env_variables(\"HUGGINGFACE_API_KEY\")"
      ],
      "metadata": {
        "id": "NK5zy7mks4Xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from torch.nn.functional import softmax\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "class ComplianceEngine:\n",
        "\n",
        "  MODEL = \"google/shieldgemma-2b\"\n",
        "\n",
        "  def __init__(self):\n",
        "    login(token=os.environ.get(\"HUGGINGFACE_API_KEY\"))\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(self.MODEL)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        self.MODEL,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    text_generation_pipeline = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer = self.tokenizer,\n",
        "        max_new_tokens=1024,\n",
        "        do_sample=False,\n",
        "        pad_token_id=self.tokenizer.eos_token_id #https://github.com/huggingface/transformers/issues/34869\n",
        "    )\n",
        "\n",
        "    self.llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
        "\n",
        "    self.text_processor = TextProcessor()\n",
        "\n",
        "    self.assessment_prompt = ChatPromptTemplate.from_template(\n",
        "        \"\"\"You are a financial compliance expert with extensive knowledge of regulatory frameworks. Your task is to evaluate whether the following transaction complies with the specified regulations.\n",
        "\n",
        "    Transaction Details:\n",
        "    {transaction}\n",
        "\n",
        "    Relevant Regulations:\n",
        "    {regulations}\n",
        "\n",
        "    Compliance Assessment Framework:\n",
        "    - Compliant: Transaction fully adheres to all applicable regulations with no reporting requirements\n",
        "    - Reporting Required: Transaction is legal but requires mandatory reporting to regulatory authorities\n",
        "    - Violation: Transaction directly contravenes one or more regulatory requirements\n",
        "\n",
        "    Step-by-step Analysis Process:\n",
        "    1. Identify the transaction type and key participants\n",
        "    2. Determine which specific regulations apply to this transaction\n",
        "    3. Assess compliance with each applicable regulation\n",
        "    4. Evaluate if reporting requirements exist\n",
        "    5. Determine final compliance status\n",
        "\n",
        "    Provide your assessment in the following JSON format:\n",
        "    {{\n",
        "        \"status\": \"Compliant\" | \"Reporting Required\" | \"Violation\",\n",
        "        \"confidence\": <float between 0 and 1>,\n",
        "        \"reasoning\": \"<concise explanation with specific regulatory references>\",\n",
        "        \"applicable_regulations\": [\"<specific regulation sections that apply>\"],\n",
        "        \"recommended_actions\": [\"<actionable steps for compliance>\"],\n",
        "        \"risk_factors\": [\"<key risk elements identified>\"]\n",
        "    }}\n",
        "\n",
        "    Return ONLY the JSON object. No additional text, explanations, or formatting. YOU WILL BE PENALIZED IF YOU RETURN ANYTHING OTHER THAN THE JSON.\n",
        "    \"\"\"\n",
        "    )\n",
        "\n",
        "    self.parser = JsonOutputParser()\n",
        "\n",
        "    # creates chain\n",
        "    self.chain = self.assessment_prompt | self.llm | self.parser\n",
        "\n",
        "\n",
        "  def retrieve_relevant_regulations(self, transaction: Transaction):\n",
        "    transaction_text = transaction.to_prompt()\n",
        "    transaction_embedding = self.text_processor.generate_embeddings(\n",
        "        [transaction_text]\n",
        "    )[0]\n",
        "\n",
        "    vector_search_stage = {\n",
        "        \"$vectorSearch\": {\n",
        "            \"index\": VECTOR_INDEX_NAME,\n",
        "            \"queryVector\": transaction_embedding,\n",
        "            \"path\": \"embedding\",\n",
        "            \"numCandidates\": 150,\n",
        "            \"limit\": 5\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Remove embedding, _id and chunks from result. As they're not neccessary at this point\n",
        "    project_stage = {\n",
        "        \"$project\":{\n",
        "            \"embedding\": 0,\n",
        "            \"chunks\": 0,\n",
        "            \"_id\": 0\n",
        "        }\n",
        "    }\n",
        "\n",
        "    pipeline = [vector_search_stage, project_stage]\n",
        "    results = list(regulations_collection.aggregate(pipeline))\n",
        "\n",
        "    regulations_text = \"\"\n",
        "    for i, reg in enumerate(results, 1):\n",
        "      regulations_text += f\"Regulation {i}: {reg['title']} ({reg['jurisdiction']}, {reg['publication_date']})\\n\"\n",
        "      regulations_text += f\"{reg['content']}\\n\\n\"\n",
        "\n",
        "    return regulations_text\n",
        "\n",
        "\n",
        "  def apply_softmax_normalization(self, assessment):\n",
        "    status_scores = {\"Compliant\": 0.0, \"Reporting Required\": 0.0, \"Violation\": 0.0}\n",
        "\n",
        "    status_scores[assessment[\"status\"]] = assessment[\"confidence\"]\n",
        "\n",
        "    scores_array = np.array(list(status_scores.values()))\n",
        "    normalized_scores = softmax(torch.tensor(scores_array), dim=0).numpy()\n",
        "\n",
        "    assessment[\"confidence\"] = float(\n",
        "            normalized_scores[list(status_scores.keys()).index(assessment[\"status\"])]\n",
        "    )\n",
        "\n",
        "    assessment[\"confidence_details\"] = {\n",
        "        status: float(score)\n",
        "        for status, score in zip(status_scores.keys(), normalized_scores)\n",
        "    }\n",
        "\n",
        "    return assessment\n",
        "\n",
        "  def assess_transaction(self, transaction: Transaction):\n",
        "    try:\n",
        "      regulations = self.retrieve_relevant_regulations(transaction)\n",
        "      print(\"-\" * 80)\n",
        "      print(f\"Retrieved {len(regulations)} relevant regulations.\")\n",
        "      print(\"Here are the first 100 characters of the first regulation:\")\n",
        "      print(regulations.split(\"\\n\")[0][:100])\n",
        "\n",
        "      inputs = {\n",
        "          \"transaction\": transaction.to_prompt(),\n",
        "          \"regulations\": regulations\n",
        "      }\n",
        "      print(\"before invoking chain\")\n",
        "      assessment = self.chain.invoke(inputs)\n",
        "      print(\"after invoking chain\")\n",
        "\n",
        "      assessment = self.apply_softmax_normalization(assessment)\n",
        "      print(\"after invoking apply_softmax_normalization\")\n",
        "\n",
        "      transaction.compliance_status = ComplianceStaus(assessment[\"status\"])\n",
        "      transaction.compliance_details = assessment\n",
        "\n",
        "      if transaction.id:\n",
        "        transaction_collection.update_one(\n",
        "            {\"_id\": transaction.id}, {\"$set\": transaction.to_dict()}\n",
        "        )\n",
        "        print(f\"Update transaction with ID: {transaction.id}\")\n",
        "      else:\n",
        "        result = transaction_collection.insert_one(transaction.to_dict())\n",
        "        transaction.id = str(result.inserted_id)\n",
        "        print(f\"Stored transaction with ID: {transaction.id}\")\n",
        "\n",
        "      return assessment\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"Error during Langchain processing: {e}\")\n",
        "      assessment = {\n",
        "          \"status\": \"Reporting Required\",\n",
        "          \"confidence\": 0.5,\n",
        "          \"reasoning\": f\"Error during assessment: {e!s}. Please review the transaction manually.\",\n",
        "          \"applicable_regulations\": [],\n",
        "          \"recommended_actions\": [\"Review transaction manually\"],\n",
        "          \"risk_factors\": [\"Assessment processing failure\"],\n",
        "      }\n",
        "      return assessment\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3a443XT3rmT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMplement Agent Orchestration with Langraph to coordinate the compliance assessment workflow"
      ],
      "metadata": {
        "id": "n13muvqx3NhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, Dict, List, Optional, TypedDict\n",
        "\n",
        "import langgraph.graph as lg\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langgraph.checkpoint.mongodb import MongoDBSaver\n",
        "\n",
        "\n",
        "# Graphs state\n",
        "class ComplianceState(TypedDict):\n",
        "  transaction: Dict[str, Any]\n",
        "  regulations: Optional[List[Dict[str, Any]]]\n",
        "  assessment: Optional[Dict[str, Any]]\n",
        "  messages: List[Union[HumanMessage, AIMessage]]\n",
        "  errors: Optional[List[str]]\n",
        "\n",
        "class ComplianceWorkflow:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.compliance_engine = ComplianceEngine()\n",
        "    self.text_processor = TextProcessor()\n",
        "\n",
        "    self.checkpoint_store = MongoDBSaver(client, DB_NAME, CHECKPOINTS)\n",
        "\n",
        "    self.workflow = self._build_graph()\n",
        "\n",
        "  def _parse_transaction(self, state:ComplianceState) -> ComplianceState:\n",
        "    try:\n",
        "      transaction_data = state[\"transaction\"]\n",
        "      transaction = Transaction(**transaction_data)\n",
        "\n",
        "      state[\"transaction\"] = transaction.to_dict()\n",
        "      state[\"messages\"].append(\n",
        "          AIMessage(content=f\"Transaction {transaction.transaction_id} parsed successfully.\")\n",
        "      )\n",
        "    except Exception as e:\n",
        "      error_msg = f\"Error parsing transaction: {e!s}\"\n",
        "      state[\"errors\"] = state.get(\"errors\", []) + [error_msg]\n",
        "      state[\"messages\"].append(AIMessage(content=error_msg))\n",
        "\n",
        "    return state\n",
        "\n",
        "  def _retrieve_regulations(self, state: ComplianceState) -> ComplianceState:\n",
        "    try:\n",
        "      transaction_data = state[\"transaction\"]\n",
        "      transaction = Transaction(**transaction_data)\n",
        "\n",
        "      regulations_text = self.compliance_engine.retrieve_relevant_regulations(\n",
        "          transaction\n",
        "      )\n",
        "\n",
        "      state[\"regulations\"] = regulations_text\n",
        "      state[\"messages\"].append(\n",
        "          AIMessage(content=\"Retrieved relevant regulations for compliance assessment\")\n",
        "      )\n",
        "\n",
        "    except Exception as e:\n",
        "      error_msg = f\"Error retrieving regulations: {e!s}\"\n",
        "      state[\"errors\"] = state.get(\"errors\", []) + [error_msg]\n",
        "      state[\"messages\"].append(AIMessage(content=error_msg))\n",
        "\n",
        "    return state\n",
        "\n",
        "  def _assess_compliance(self, state: ComplianceState) -> ComplianceState:\n",
        "    try:\n",
        "      transaction_data = state[\"transaction\"]\n",
        "      transaction = Transaction(**transaction_data)\n",
        "\n",
        "      assessment = self.compliance_engine.assess_transaction(transaction)\n",
        "\n",
        "      state[\"assessment\"] = assessment\n",
        "      state[\"transaction\"] = (\n",
        "          transaction.to_dict()\n",
        "      )\n",
        "      summary = f\"Compliance assessment complete. Status: {assessment['status']} (Confidence: {assessment['confidence']:.2f})\\n\"\n",
        "      summary += f\"Reasoning: {assessment['reasoning']} \\n\"\n",
        "      if assessment.get(\"recommended_actions\"):\n",
        "        summary += f\"Recommended actions: {', '.join(assessment['recommended_actions'])}\\n\"\n",
        "\n",
        "      state[\"messages\"].append(AIMessage(content=summary))\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error assessing compliance: {e!s}\"\n",
        "        state[\"errors\"] = state.get(\"errors\", []) + [error_msg]\n",
        "        state[\"messages\"].append(AIMessage(content=error_msg))\n",
        "\n",
        "    return state\n",
        "\n",
        "  def _should_retry(self, state: ComplianceState) -> str:\n",
        "    if state.get(\"errors\") and len(state[\"errors\"]) < 3:\n",
        "        return \"retry\"\n",
        "    return \"end\"\n",
        "\n",
        "  def _build_graph(self):\n",
        "\n",
        "    builder = lg.StateGraph(ComplianceState)\n",
        "\n",
        "    # nodes\n",
        "    builder.add_node(\"parse_transaction\", self._parse_transaction)\n",
        "    builder.add_node(\"retrieve_regulations\", self._retrieve_regulations)\n",
        "    builder.add_node(\"assess_compliance\", self._assess_compliance)\n",
        "\n",
        "    # edges\n",
        "    builder.add_edge(\"parse_transaction\", \"retrieve_regulations\")\n",
        "    builder.add_edge(\"retrieve_regulations\", \"assess_compliance\")\n",
        "\n",
        "    # conditional edge for error handling\n",
        "    builder.add_conditional_edges(\n",
        "        \"assess_compliance\",\n",
        "        self._should_retry,\n",
        "        {\"retry\": \"parse_transaction\", \"end\": lg.END}\n",
        "    )\n",
        "\n",
        "    builder.set_entry_point(\"parse_transaction\")\n",
        "\n",
        "    return builder.compile(checkpointer=self.checkpoint_store)\n",
        "\n",
        "  def process_transaction(self, transaction_data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    initial_state = ComplianceState(\n",
        "        transaction=transaction_data,\n",
        "        regulations=None,\n",
        "        assessment=None,\n",
        "        messages=[\n",
        "            HumanMessage(\n",
        "                content=f\"Process transaction {transaction_data.get('transaction_id', 'unknown')}\"\n",
        "            )\n",
        "        ],\n",
        "        errors=None,\n",
        "    )\n",
        "\n",
        "    config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
        "    final_state = self.workflow.invoke(initial_state, config)\n",
        "\n",
        "    return final_state\n"
      ],
      "metadata": {
        "id": "keXDumM33VPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample transactions for demonstration\n",
        "sample_transactions = [\n",
        "    {\n",
        "        \"transaction_id\": \"TX123456789\",\n",
        "        \"amount\": 150000.00,\n",
        "        \"currency\": \"EUR\",\n",
        "        \"sender\": {\n",
        "            \"name\": \"European Trading Ltd\",\n",
        "            \"country\": \"Germany\",\n",
        "            \"account_number\": \"DE89370400440532013000\",\n",
        "            \"institution\": \"Deutsche Bank\",\n",
        "            \"is_sanctioned\": False,\n",
        "        },\n",
        "        \"receiver\": {\n",
        "            \"name\": \"Global Imports Inc\",\n",
        "            \"country\": \"United States\",\n",
        "            \"account_number\": \"US12345678901234567890\",\n",
        "            \"institution\": \"Bank of America\",\n",
        "            \"is_sanctioned\": False,\n",
        "        },\n",
        "        \"transaction_date\": \"2023-11-15\",\n",
        "        \"transaction_type\": \"International Wire Transfer\",\n",
        "        \"description\": \"Payment for machinery parts\",\n",
        "    },\n",
        "    {\n",
        "        \"transaction_id\": \"TX987654321\",\n",
        "        \"amount\": 75000.00,\n",
        "        \"currency\": \"USD\",\n",
        "        \"sender\": {\n",
        "            \"name\": \"American Exports LLC\",\n",
        "            \"country\": \"United States\",\n",
        "            \"account_number\": \"US98765432109876543210\",\n",
        "            \"institution\": \"JP Morgan Chase\",\n",
        "            \"is_sanctioned\": False,\n",
        "        },\n",
        "        \"receiver\": {\n",
        "            \"name\": \"Tehran Trading Co\",\n",
        "            \"country\": \"Iran\",\n",
        "            \"account_number\": \"IR123456789012345678901234\",\n",
        "            \"institution\": \"Bank Melli Iran\",\n",
        "            \"is_sanctioned\": True,\n",
        "        },\n",
        "        \"transaction_date\": \"2023-12-01\",\n",
        "        \"transaction_type\": \"International Wire Transfer\",\n",
        "        \"description\": \"Consulting services\",\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "K1o4L8GfAuPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "workflow = ComplianceWorkflow()\n",
        "\n",
        "results = []\n",
        "for tx_data in sample_transactions:\n",
        "    print(f\"\\nProcessing transaction {tx_data['transaction_id']}...\")\n",
        "    result = workflow.process_transaction(tx_data)\n",
        "    results.append(result)\n",
        "\n",
        "    for message in result[\"messages\"]:\n",
        "        if isinstance(message, AIMessage):\n",
        "            print(f\"System: {message.content}\")\n",
        "\n",
        "    if result.get(\"assessment\"):\n",
        "        assessment = result[\"assessment\"]\n",
        "        print(f\"\\nFinal Assessment for {tx_data['transaction_id']}:\")\n",
        "        print(f\"Status: {assessment['status']}\")\n",
        "        print(f\"Confidence: {assessment['confidence']:.2f}\")\n",
        "        print(f\"Reasoning: {assessment['reasoning']}\")\n",
        "        if assessment.get(\"risk_factors\"):\n",
        "            print(f\"Risk Factors: {', '.join(assessment['risk_factors'])}\")\n",
        "        if assessment.get(\"applicable_regulations\"):\n",
        "            print(\n",
        "                f\"Applicable Regulations: {', '.join(assessment['applicable_regulations'])}\"\n",
        "            )\n",
        "        if assessment.get(\"recommended_actions\"):\n",
        "            print(\n",
        "                f\"Recommended Actions: {', '.join(assessment['recommended_actions'])}\"\n",
        "            )\n",
        "        print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "mHGUMebcAzmT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}